{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4575c8d",
   "metadata": {},
   "source": [
    "# **Datasets Cleansing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1c8771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guanq\\AppData\\Local\\Temp\\ipykernel_35952\\3172591061.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2433b81",
   "metadata": {},
   "source": [
    "**Our group used six datasets from '*DATA.NY.GOV*'(https://data.ny.gov/).**\n",
    "\n",
    "**Congestion Zone:** https://data.ny.gov/Transportation/MTA-Congestion-Relief-Zone-Vehicle-Entries-Beginni/t6yz-b64h/about_data .\n",
    "\n",
    "**Daily ridership:** https://data.ny.gov/Transportation/MTA-Daily-Ridership-and-Traffic-Beginning-2020/sayj-mze2/about_data .\n",
    "\n",
    "**Tunnel and bridge crossings:** https://data.ny.gov/Transportation/MTA-Bridges-and-Tunnels-Hourly-Crossings-Beginning/ebfx-2m7v/about_data .\n",
    "\n",
    "**Fare Evasion:** https://data.ny.gov/Transportation/MTA-NYCT-Subway-Fare-Evasion-Beginning-2018/6kj3-ijvb/about_data .\n",
    "\n",
    "**Station entry/exit:** https://data.ny.gov/Transportation/MTA-Subway-Entrances-and-Exits-2024/i9wp-a4ja/about_data .\n",
    "\n",
    "**Traffic violations:** https://data.ny.gov/Transportation/Traffic-Tickets-Issued-Four-Year-Window/q4hy-kbtf/about_data .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9198bbb-0e84-4650-8edb-d452d54bea80",
   "metadata": {},
   "source": [
    "## **1. Congestion Zone cleansing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaddfcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 missing values\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"congestion zone.csv\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = df.isnull().sum()\n",
    "total_missing = missing_count.sum()\n",
    "print(f\"\\nFound {total_missing} missing values\")\n",
    "if total_missing > 0:\n",
    "    print(\"Missing values by column:\")\n",
    "    print(missing_count[missing_count > 0])\n",
    "\n",
    "# Handle missing values\n",
    "if total_missing > 0:\n",
    "    # For numeric columns, fill with median\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "            print(f\"Filled missing values in {col} with median\")\n",
    "\n",
    "    # For categorical columns, fill with mode\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in cat_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            mode_val = df[col].mode()[0]\n",
    "            df[col] = df[col].fillna(mode_val)\n",
    "            print(f\"Filled missing values in {col} with mode: '{mode_val}'\")\n",
    "\n",
    "    # Verify missing values have been handled\n",
    "    remaining_missing = df.isnull().sum().sum()\n",
    "    print(f\"Remaining missing values: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321fe9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Check for and remove duplicates\n",
    "dup_count = df.duplicated().sum()\n",
    "print(f\"\\nFound {dup_count} duplicate rows\")\n",
    "\n",
    "if dup_count > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {dup_count} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9f3914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Toll Date to datetime\n",
      "Converted Minute of Hour to numeric\n",
      "Converted Hour of Day to numeric\n",
      "Converted CRZ Entries to numeric\n"
     ]
    }
   ],
   "source": [
    "# Convert necessary data types\n",
    "# Convert date columns to datetime\n",
    "date_cols = [col for col in df.columns if 'Date' in col or 'date' in col]\n",
    "for col in date_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        print(f\"Converted {col} to datetime\")\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_cols = ['Minute of Hour', 'Hour of Day', 'CRZ Entries']\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        print(f\"Converted {col} to numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abc0394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identified 58540 outliers in CRZ Entries\n",
      "Lower bound: -87.0, Upper bound: 145.0\n",
      "\n",
      "Summary statistics before removing outliers:\n",
      "count    508032.000000\n",
      "mean         45.857869\n",
      "std          80.247676\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           4.000000\n",
      "75%          58.000000\n",
      "max         599.000000\n",
      "Name: CRZ Entries, dtype: float64\n",
      "\n",
      "Summary statistics after removing outliers:\n",
      "count    449492.000000\n",
      "mean         21.378494\n",
      "std          36.253207\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           3.000000\n",
      "75%          24.000000\n",
      "max         145.000000\n",
      "Name: CRZ Entries, dtype: float64\n",
      "\n",
      "Removed 58540 outliers. Final data shape: (449492, 14)\n"
     ]
    }
   ],
   "source": [
    "# Detect outliers in 'CRZ Entries'\n",
    "if 'CRZ Entries' in df.columns:\n",
    "    # Calculate quartiles\n",
    "    Q1 = df['CRZ Entries'].quantile(0.25)\n",
    "    Q3 = df['CRZ Entries'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define outlier boundaries\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = (df['CRZ Entries'] < lower_bound) | (df['CRZ Entries'] > upper_bound)\n",
    "    outlier_count = outliers.sum()\n",
    "    print(f\"\\nIdentified {outlier_count} outliers in CRZ Entries\")\n",
    "    print(f\"Lower bound: {lower_bound}, Upper bound: {upper_bound}\")\n",
    "    \n",
    "    # Summary before removing outliers\n",
    "    print(\"\\nSummary statistics before removing outliers:\")\n",
    "    print(df['CRZ Entries'].describe())\n",
    "    \n",
    "    # Remove outliers\n",
    "    df_clean = df[~outliers]\n",
    "    \n",
    "    # Summary after removing outliers\n",
    "    print(\"\\nSummary statistics after removing outliers:\")\n",
    "    print(df_clean['CRZ Entries'].describe())\n",
    "    \n",
    "    print(f\"\\nRemoved {outlier_count} outliers. Final data shape: {df_clean.shape}\")\n",
    "else:\n",
    "    print(\"'CRZ Entries' column not found\")\n",
    "    df_clean = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb59bc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Data Quality Report:\n",
      "Total rows: 449492\n",
      "Missing values: 0\n",
      "Duplicate row count: 0\n",
      "\n",
      "Saved cleaned data to 'cleaned_congestion_zone.csv'\n"
     ]
    }
   ],
   "source": [
    "# Final data quality check\n",
    "print(\"\\nFinal Data Quality Report:\")\n",
    "print(f\"Total rows: {len(df_clean)}\")\n",
    "print(f\"Missing values: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate row count: {df_clean.duplicated().sum()}\")\n",
    "\n",
    "df_clean.to_csv(\"cleaned_congestion_zone.csv\", index=False)\n",
    "print(\"\\nSaved cleaned data to 'cleaned_congestion_zone.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931defb4",
   "metadata": {},
   "source": [
    "## **2. Daily Ridership Cleansing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca12626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 32 missing values\n",
      "Missing values by column:\n",
      "Count    32\n",
      "dtype: int64\n",
      "Filled missing values in Count with median\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"Daily ridership.csv\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_count1 = df1.isnull().sum()\n",
    "total_missing1 = missing_count1.sum()\n",
    "print(f\"\\nFound {total_missing1} missing values\")\n",
    "if total_missing1 > 0:\n",
    "    print(\"Missing values by column:\")\n",
    "    print(missing_count1[missing_count1 > 0])\n",
    "\n",
    "# Handle missing values\n",
    "if total_missing1 > 0:\n",
    "    # For numeric columns, fill with median\n",
    "    numeric_cols1 = df1.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col1 in numeric_cols1:\n",
    "        if df1[col1].isnull().sum() > 0:\n",
    "            df1[col1] = df1[col1].fillna(df1[col1].median())\n",
    "            print(f\"Filled missing values in {col1} with median\")\n",
    "    \n",
    "    # For categorical columns, fill with mode\n",
    "    cat_cols1 = df1.select_dtypes(include=['object']).columns\n",
    "    for col1 in cat_cols1:\n",
    "        if df1[col1].isnull().sum() > 0:\n",
    "            mode_val1 = df1[col1].mode()[0]\n",
    "            df1[col1] = df1[col1].fillna(mode_val1)\n",
    "            print(f\"Filled missing values in {col1} with mode: '{mode_val1}'\")\n",
    "    \n",
    "    # Verify missing values have been handled\n",
    "    remaining_missing1 = df1.isnull().sum().sum()\n",
    "    print(f\"Remaining missing values: {remaining_missing1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f6d2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Check for and remove duplicates\n",
    "dup_count1 = df1.duplicated().sum()\n",
    "print(f\"\\nFound {dup_count1} duplicate rows\")\n",
    "\n",
    "if dup_count1 > 0:\n",
    "    df1 = df1.drop_duplicates()\n",
    "    print(f\"Removed {dup_count1} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a3738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Date to datetime\n",
      "Converted Count to numeric\n"
     ]
    }
   ],
   "source": [
    "# Convert necessary data types\n",
    "# Convert Date to datetime\n",
    "if 'Date' in df1.columns:\n",
    "    df1['Date'] = pd.to_datetime(df1['Date'], errors='coerce')\n",
    "    print(\"Converted Date to datetime\")\n",
    "\n",
    "# Ensure Count is numeric\n",
    "if 'Count' in df1.columns:\n",
    "    df1['Count'] = pd.to_numeric(df1['Count'], errors='coerce')\n",
    "    print(\"Converted Count to numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8dd9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlier detection by mode:\n",
      "Mode AAR: 2 outliers\n",
      "  Bounds: -1222.0 - 46322.0\n",
      "Mode SIR: 41 outliers\n",
      "  Bounds: -4770.0 - 13918.0\n",
      "Mode MNR: 0 outliers\n",
      "  Bounds: -133665.0 - 372663.0\n",
      "Mode Subway: 0 outliers\n",
      "  Bounds: -887492.0 - 6128636.0\n",
      "Mode LIRR: 0 outliers\n",
      "  Bounds: -103329.0 - 388815.0\n",
      "Mode Bus: 0 outliers\n",
      "  Bounds: -218282.0 - 2296790.0\n",
      "Mode BT: 117 outliers\n",
      "  Bounds: 612878.0 - 1146910.0\n",
      "Mode CBD Entries: 3 outliers\n",
      "  Bounds: 462565.5 - 624145.5\n",
      "Mode CRZ Entries: 3 outliers\n",
      "  Bounds: 406000.625 - 554713.625\n",
      "\n",
      "Total outliers across all modes: 166\n",
      "\n",
      "Summary statistics before removing outliers:\n",
      "count    1.285900e+04\n",
      "mean     6.738839e+05\n",
      "std      9.633814e+05\n",
      "min      2.000000e+00\n",
      "25%      2.417750e+04\n",
      "50%      1.853480e+05\n",
      "75%      9.500055e+05\n",
      "max      5.498809e+06\n",
      "Name: Count, dtype: float64\n",
      "\n",
      "Summary statistics after removing outliers:\n",
      "count    1.269300e+04\n",
      "mean     6.778974e+05\n",
      "std      9.688226e+05\n",
      "min      2.000000e+00\n",
      "25%      2.397200e+04\n",
      "50%      1.823140e+05\n",
      "75%      9.539330e+05\n",
      "max      5.498809e+06\n",
      "Name: Count, dtype: float64\n",
      "\n",
      "Removed 166 outliers. Final data shape: (12693, 3)\n"
     ]
    }
   ],
   "source": [
    "# Detect outliers in 'Count'\n",
    "if 'Count' in df1.columns:\n",
    "    modes1 = df1['Mode'].unique()\n",
    "    outliers1 = pd.Series(False, index=df1.index)\n",
    "    \n",
    "    print(\"\\nOutlier detection by mode:\")\n",
    "    for mode1 in modes1:\n",
    "        mode_data1 = df1[df1['Mode'] == mode1]['Count']\n",
    "        if len(mode_data1) >= 4:  \n",
    "            Q1_1 = mode_data1.quantile(0.25)\n",
    "            Q3_1 = mode_data1.quantile(0.75)\n",
    "            IQR1 = Q3_1 - Q1_1\n",
    "            \n",
    "            lower_bound1 = Q1_1 - 1.5 * IQR1\n",
    "            upper_bound1 = Q3_1 + 1.5 * IQR1\n",
    "            \n",
    "            mode_outliers1 = (mode_data1 < lower_bound1) | (mode_data1 > upper_bound1)\n",
    "            outlier_count1 = mode_outliers1.sum()\n",
    "            \n",
    "            print(f\"Mode {mode1}: {outlier_count1} outliers\")\n",
    "            print(f\"  Bounds: {lower_bound1} - {upper_bound1}\")\n",
    "            \n",
    "            outliers1 = outliers1 | (df1['Mode'] == mode1) & mode_outliers1\n",
    "    \n",
    "    total_outliers1 = outliers1.sum()\n",
    "    print(f\"\\nTotal outliers across all modes: {total_outliers1}\")\n",
    "    \n",
    "    # Summary before removing outliers\n",
    "    print(\"\\nSummary statistics before removing outliers:\")\n",
    "    print(df1['Count'].describe())\n",
    "    \n",
    "    # Remove outliers\n",
    "    df_clean1 = df1[~outliers1]\n",
    "    \n",
    "    # Summary after removing outliers\n",
    "    print(\"\\nSummary statistics after removing outliers:\")\n",
    "    print(df_clean1['Count'].describe())\n",
    "    \n",
    "    print(f\"\\nRemoved {total_outliers1} outliers. Final data shape: {df_clean1.shape}\")\n",
    "else:\n",
    "    print(\"'Count' column not found\")\n",
    "    df_clean1 = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76c01f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Data Quality Report:\n",
      "Total rows: 12693\n",
      "Missing values: 0\n",
      "Duplicate row count: 0\n",
      "\n",
      "Distribution by Mode:\n",
      "Mode\n",
      "MNR            1825\n",
      "Subway         1825\n",
      "LIRR           1825\n",
      "Bus            1825\n",
      "AAR            1823\n",
      "SIR            1784\n",
      "BT             1708\n",
      "CBD Entries      39\n",
      "CRZ Entries      39\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved cleaned data to 'cleaned_daily_ridership.csv'\n"
     ]
    }
   ],
   "source": [
    "# Final data quality check\n",
    "print(\"\\nFinal Data Quality Report:\")\n",
    "print(f\"Total rows: {len(df_clean1)}\")\n",
    "print(f\"Missing values: {df_clean1.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate row count: {df_clean1.duplicated().sum()}\")\n",
    "\n",
    "# Distribution by Mode\n",
    "if 'Mode' in df_clean1.columns:\n",
    "    print(\"\\nDistribution by Mode:\")\n",
    "    print(df_clean1['Mode'].value_counts())\n",
    "\n",
    "df_clean1.to_csv(\"cleaned_daily_ridership.csv\", index=False)\n",
    "print(\"\\nSaved cleaned data to 'cleaned_daily_ridership.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4a867-9691-481e-96b4-797cf67c6051",
   "metadata": {},
   "source": [
    "## **3. Tunning and Bridge crossings cleansing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5d99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 missing values\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"Tunnel and Bridge Crossings.csv\")\n",
    "\n",
    "\n",
    "# Check for missing values\n",
    "missing_count2 = df2.isnull().sum()\n",
    "total_missing2 = missing_count2.sum()\n",
    "print(f\"\\nFound {total_missing2} missing values\")\n",
    "if total_missing2 > 0:\n",
    "    print(\"Missing values by column:\")\n",
    "    print(missing_count2[missing_count2 > 0])\n",
    "\n",
    "# Handle missing values\n",
    "if total_missing2 > 0:\n",
    "    # For numeric columns, fill with median\n",
    "    numeric_cols2 = df2.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col2 in numeric_cols2:\n",
    "        if df2[col2].isnull().sum() > 0:\n",
    "            df2[col2] = df2[col2].fillna(df2[col2].median())\n",
    "            print(f\"Filled missing values in {col2} with median\")\n",
    "    \n",
    "    # For categorical columns, fill with mode\n",
    "    cat_cols2 = df2.select_dtypes(include=['object']).columns\n",
    "    for col2 in cat_cols2:\n",
    "        if df2[col2].isnull().sum() > 0:\n",
    "            mode_val2 = df2[col2].mode()[0]\n",
    "            df2[col2] = df2[col2].fillna(mode_val2)\n",
    "            print(f\"Filled missing values in {col2} with mode: '{mode_val2}'\")\n",
    "    \n",
    "    # Verify missing values have been handled\n",
    "    remaining_missing2 = df2.isnull().sum().sum()\n",
    "    print(f\"Remaining missing values: {remaining_missing2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f95a9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 0 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Check for and remove duplicates\n",
    "dup_count2 = df2.duplicated().sum()\n",
    "print(f\"\\nFound {dup_count2} duplicate rows\")\n",
    "\n",
    "if dup_count2 > 0:\n",
    "    df2 = df2.drop_duplicates()\n",
    "    print(f\"Removed {dup_count2} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad5682bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Date to datetime\n",
      "Converted Traffic Count to numeric\n"
     ]
    }
   ],
   "source": [
    "# Convert necessary data types\n",
    "# Convert date columns to datetime\n",
    "date_cols2 = [col2 for col2 in df2.columns if 'Date' in col2 or 'date' in col2]\n",
    "for col2 in date_cols2:\n",
    "    df2[col2] = pd.to_datetime(df2[col2], errors='coerce')\n",
    "    print(f\"Converted {col2} to datetime\")\n",
    "\n",
    "# Convert Traffic Count to numeric\n",
    "if 'Traffic Count' in df2.columns:\n",
    "    df2['Traffic Count'] = pd.to_numeric(df2['Traffic Count'], errors='coerce')\n",
    "    print(\"Converted Traffic Count to numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a1a38bf-91e1-4acd-b327-263c02e2523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identified 1941497 outliers in Traffic Count\n",
      "Lower bound: -38.5, Upper bound: 69.5\n",
      "\n",
      "Summary statistics before removing outliers:\n",
      "count    1.076610e+07\n",
      "mean     1.730310e+02\n",
      "std      6.067171e+02\n",
      "min      1.000000e+00\n",
      "25%      2.000000e+00\n",
      "50%      6.000000e+00\n",
      "75%      2.900000e+01\n",
      "max      1.452200e+04\n",
      "Name: Traffic Count, dtype: float64\n",
      "\n",
      "Summary statistics after removing outliers:\n",
      "count    8.824599e+06\n",
      "mean     9.229330e+00\n",
      "std      1.282092e+01\n",
      "min      1.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      4.000000e+00\n",
      "75%      1.100000e+01\n",
      "max      6.900000e+01\n",
      "Name: Traffic Count, dtype: float64\n",
      "\n",
      "Removed 1941497 outliers. Final data shape: (8824599, 11)\n"
     ]
    }
   ],
   "source": [
    "# Detect outliers in 'Traffic Count'\n",
    "if 'Traffic Count' in df2.columns:\n",
    "    # Calculate quartiles\n",
    "    Q1_2 = df2['Traffic Count'].quantile(0.25)\n",
    "    Q3_2 = df2['Traffic Count'].quantile(0.75)\n",
    "    IQR2 = Q3_2 - Q1_2\n",
    "    \n",
    "    # Define outlier boundaries\n",
    "    lower_bound2 = Q1_2 - 1.5 * IQR2\n",
    "    upper_bound2 = Q3_2 + 1.5 * IQR2\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers2 = (df2['Traffic Count'] < lower_bound2) | (df2['Traffic Count'] > upper_bound2)\n",
    "    outlier_count2 = outliers2.sum()\n",
    "    print(f\"\\nIdentified {outlier_count2} outliers in Traffic Count\")\n",
    "    print(f\"Lower bound: {lower_bound2}, Upper bound: {upper_bound2}\")\n",
    "  \n",
    "    print(\"\\nSummary statistics before removing outliers:\")\n",
    "    print(df2['Traffic Count'].describe())\n",
    "    \n",
    "    # Remove outliers\n",
    "    df_clean2 = df2[~outliers2]\n",
    "    \n",
    "    # Summary after removing outliers\n",
    "    print(\"\\nSummary statistics after removing outliers:\")\n",
    "    print(df_clean2['Traffic Count'].describe())\n",
    "    \n",
    "    print(f\"\\nRemoved {outlier_count2} outliers. Final data shape: {df_clean2.shape}\")\n",
    "else:\n",
    "    print(\"'Traffic Count' column not found\")\n",
    "    df_clean2 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40907571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Data Quality Report:\n",
      "Total rows: 8824599\n",
      "Missing values: 0\n",
      "Duplicate row count: 0\n",
      "\n",
      "Saved cleaned data to 'cleaned_tunnel_bridge_crossings.csv'\n"
     ]
    }
   ],
   "source": [
    "# Final data quality check\n",
    "print(\"\\nFinal Data Quality Report:\")\n",
    "print(f\"Total rows: {len(df_clean2)}\")\n",
    "print(f\"Missing values: {df_clean2.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate row count: {df_clean2.duplicated().sum()}\")\n",
    "\n",
    "df_clean2.to_csv(\"cleaned_tunnel_bridge_crossings.csv\", index=False)\n",
    "print(\"\\nSaved cleaned data to 'cleaned_tunnel_bridge_crossings.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9828a-8894-4644-a80b-28684e74597c",
   "metadata": {},
   "source": [
    "## **4. Fare Evasion cleansing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f861a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df3 = pd.read_csv(\"Fare evasion.csv\")\n",
    "\n",
    "# Standardize column names\n",
    "df3.columns = df3.columns.str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Process Time Period column\n",
    "if 'Time_Period' in df3.columns:\n",
    "    # Extract year and quarter\n",
    "    df3['Year3'] = df3['Time_Period'].str.extract(r'(\\d{4})').astype(int)\n",
    "    df3['Quarter3'] = df3['Time_Period'].str.extract(r'Q(\\d)').astype(int)\n",
    "    \n",
    "    # Calculate a decimal date for time series analysis \n",
    "    df3['Decimal_Year3'] = df3['Year3'] + (df3['Quarter3'] - 1) * 0.25\n",
    "    \n",
    "    # Create a proper datetime for the start of each quarter\n",
    "    df3['Date3'] = pd.to_datetime(df3['Year3'].astype(str) + '-' + \n",
    "                                (df3['Quarter3'] * 3 - 2).astype(str) + '-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42872dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated missing Fare_Evasion values\n",
      "Filled missing Margin_of_Error values with average: 0.0112\n",
      "Filled 10 missing values\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "missing_before3 = df3.isnull().sum().sum()\n",
    "if 'Fare_Evasion' in df3.columns and df3['Fare_Evasion'].isnull().any():\n",
    "    df3['Fare_Evasion'] = df3['Fare_Evasion'].interpolate(method='linear')\n",
    "    print(\"Interpolated missing Fare_Evasion values\")\n",
    "\n",
    "if 'Margin_of_Error' in df3.columns and df3['Margin_of_Error'].isnull().any():\n",
    "    available_margins3 = df3['Margin_of_Error'].dropna()\n",
    "    if len(available_margins3) > 0:\n",
    "        avg_margin3 = available_margins3.mean()\n",
    "        df3['Margin_of_Error'] = df3['Margin_of_Error'].fillna(avg_margin3)\n",
    "        print(f\"Filled missing Margin_of_Error values with average: {avg_margin3:.4f}\")\n",
    "\n",
    "missing_after3 = df3.isnull().sum().sum()\n",
    "print(f\"Filled {missing_before3 - missing_after3} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ae047d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 outliers, data shape after removal: (28, 7)\n"
     ]
    }
   ],
   "source": [
    "# Detect and remove outliers\n",
    "if 'Fare_Evasion' in df3.columns:\n",
    "    # Calculate quartiles and IQR\n",
    "    Q1_3 = df3['Fare_Evasion'].quantile(0.25)\n",
    "    Q3_3 = df3['Fare_Evasion'].quantile(0.75)\n",
    "    IQR3 = Q3_3 - Q1_3\n",
    "    \n",
    "    # Define outlier boundaries\n",
    "    lower_bound3 = Q1_3 - 1.5 * IQR3\n",
    "    upper_bound3 = Q3_3 + 1.5 * IQR3\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers3 = (df3['Fare_Evasion'] < lower_bound3) | (df3['Fare_Evasion'] > upper_bound3)\n",
    "    outlier_count3 = outliers3.sum()\n",
    "    \n",
    "    # Save the time periods with outliers for reference\n",
    "    if outlier_count3 > 0:\n",
    "        outlier_periods3 = df3.loc[outliers3, 'Time_Period'].tolist()\n",
    "        print(f\"Identified {outlier_count3} outliers in the following time periods: {outlier_periods3}\")\n",
    "    \n",
    "    # Remove outliers\n",
    "    df3_original = df3.copy() \n",
    "    df3 = df3[~outliers3]\n",
    "    print(f\"Removed {outlier_count3} outliers, data shape after removal: {df3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bf47f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Upper_Bound3 and Lower_Bound3 columns\n",
      "Added COVID_Period3 classification\n",
      "Added YoY_Change3 column (percentage change from same quarter previous year)\n",
      "Added QoQ_Change3 column (percentage change from previous quarter)\n"
     ]
    }
   ],
   "source": [
    "# Add derived features for analysis\n",
    "if all(col in df3.columns for col in ['Fare_Evasion', 'Margin_of_Error']):\n",
    "    df3['Upper_Bound3'] = df3['Fare_Evasion'] + df3['Margin_of_Error']\n",
    "    df3['Lower_Bound3'] = df3['Fare_Evasion'] - df3['Margin_of_Error']\n",
    "    df3['Lower_Bound3'] = df3['Lower_Bound3'].apply(lambda x: max(0, x))\n",
    "    print(\"Added Upper_Bound3 and Lower_Bound3 columns\")\n",
    "\n",
    "# Add categorical period features for comparison\n",
    "if 'Year3' in df3.columns:\n",
    "    df3['COVID_Period3'] = 'Pre-COVID'\n",
    "    df3.loc[(df3['Year3'] == 2020) & (df3['Quarter3'] >= 1) | \n",
    "            (df3['Year3'] == 2021) & (df3['Quarter3'] <= 2), 'COVID_Period3'] = 'During-COVID'\n",
    "    df3.loc[(df3['Year3'] > 2021) | \n",
    "            (df3['Year3'] == 2021) & (df3['Quarter3'] > 2), 'COVID_Period3'] = 'Post-COVID'\n",
    "    print(\"Added COVID_Period3 classification\")\n",
    "\n",
    "# Add year-over-year change\n",
    "if 'Fare_Evasion' in df3.columns and 'Quarter3' in df3.columns:\n",
    "    df3 = df3.sort_values(['Year3', 'Quarter3'])\n",
    "    \n",
    "    df3['YoY_Change3'] = df3.groupby('Quarter3')['Fare_Evasion'].pct_change(4) * 100\n",
    "    print(\"Added YoY_Change3 column (percentage change from same quarter previous year)\")\n",
    "\n",
    "# Add quarter-over-quarter change\n",
    "if 'Fare_Evasion' in df3.columns:\n",
    "    df3['QoQ_Change3'] = df3['Fare_Evasion'].pct_change() * 100\n",
    "    print(\"Added QoQ_Change3 column (percentage change from previous quarter)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae1e6c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Fare_Evasion_Pct column with percentage format\n",
      "Added Margin_of_Error_Pct column with percentage format\n",
      "Added Upper_Bound3_Pct column with percentage format\n",
      "Added Lower_Bound3_Pct column with percentage format\n"
     ]
    }
   ],
   "source": [
    "# Format percentage columns for better readability\n",
    "percentage_cols3 = ['Fare_Evasion', 'Margin_of_Error', 'Upper_Bound3', 'Lower_Bound3']\n",
    "for col3 in percentage_cols3:\n",
    "    if col3 in df3.columns:\n",
    "        df3[f'{col3}_Pct'] = df3[col3] * 100\n",
    "        print(f\"Added {col3}_Pct column with percentage format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22effb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yearly statistics generated\n",
      "\n",
      "COVID period statistics generated\n",
      "\n",
      "Final cleaned data shape: (28, 16)\n",
      "\n",
      "Column Data Types:\n",
      "Time_Period                    object\n",
      "Fare_Evasion                  float64\n",
      "Margin_of_Error               float64\n",
      "Year3                           int32\n",
      "Quarter3                        int32\n",
      "Decimal_Year3                 float64\n",
      "Date3                  datetime64[ns]\n",
      "Upper_Bound3                  float64\n",
      "Lower_Bound3                  float64\n",
      "COVID_Period3                  object\n",
      "YoY_Change3                   float64\n",
      "QoQ_Change3                   float64\n",
      "Fare_Evasion_Pct              float64\n",
      "Margin_of_Error_Pct           float64\n",
      "Upper_Bound3_Pct              float64\n",
      "Lower_Bound3_Pct              float64\n",
      "dtype: object\n",
      "\n",
      "Sample of cleaned data:\n",
      "  Time_Period  Fare_Evasion  Margin_of_Error  Year3  Quarter3  Decimal_Year3  \\\n",
      "0     2018-Q1         0.028         0.011158   2018         1        2018.00   \n",
      "1     2018-Q2         0.029         0.011158   2018         2        2018.25   \n",
      "2     2018-Q3         0.038         0.011158   2018         3        2018.50   \n",
      "3     2018-Q4         0.034         0.011158   2018         4        2018.75   \n",
      "4     2019-Q1         0.039         0.011158   2019         1        2019.00   \n",
      "\n",
      "       Date3  Upper_Bound3  Lower_Bound3 COVID_Period3  YoY_Change3  \\\n",
      "0 2018-01-01      0.039158      0.016842     Pre-COVID          NaN   \n",
      "1 2018-04-01      0.040158      0.017842     Pre-COVID          NaN   \n",
      "2 2018-07-01      0.049158      0.026842     Pre-COVID          NaN   \n",
      "3 2018-10-01      0.045158      0.022842     Pre-COVID          NaN   \n",
      "4 2019-01-01      0.050158      0.027842     Pre-COVID          NaN   \n",
      "\n",
      "   QoQ_Change3  Fare_Evasion_Pct  Margin_of_Error_Pct  Upper_Bound3_Pct  \\\n",
      "0          NaN               2.8             1.115789          3.915789   \n",
      "1     3.571429               2.9             1.115789          4.015789   \n",
      "2    31.034483               3.8             1.115789          4.915789   \n",
      "3   -10.526316               3.4             1.115789          4.515789   \n",
      "4    14.705882               3.9             1.115789          5.015789   \n",
      "\n",
      "   Lower_Bound3_Pct  \n",
      "0          1.684211  \n",
      "1          1.784211  \n",
      "2          2.684211  \n",
      "3          2.284211  \n",
      "4          2.784211  \n",
      "\n",
      "Statistics comparison before and after outlier removal:\n",
      "Before removal: count    28.0000\n",
      "mean      0.0954\n",
      "std       0.0402\n",
      "min       0.0280\n",
      "25%       0.0550\n",
      "50%       0.1085\n",
      "75%       0.1315\n",
      "max       0.1400\n",
      "Name: Fare_Evasion, dtype: float64\n",
      "After removal: count    28.0000\n",
      "mean      0.0954\n",
      "std       0.0402\n",
      "min       0.0280\n",
      "25%       0.0550\n",
      "50%       0.1085\n",
      "75%       0.1315\n",
      "max       0.1400\n",
      "Name: Fare_Evasion, dtype: float64\n",
      "\n",
      "Data Quality Report:\n",
      "Total rows: 28\n",
      "Missing values: 17\n",
      "\n",
      "Fare Evasion Summary Statistics:\n",
      "count    28.000000\n",
      "mean      0.095357\n",
      "std       0.040239\n",
      "min       0.028000\n",
      "25%       0.055000\n",
      "50%       0.108500\n",
      "75%       0.131500\n",
      "max       0.140000\n",
      "Name: Fare_Evasion, dtype: float64\n",
      "\n",
      "Year-over-Year Summary:\n",
      "Year3\n",
      "2018       NaN\n",
      "2019       NaN\n",
      "2020       NaN\n",
      "2021       NaN\n",
      "2022    304.20\n",
      "2023    179.26\n",
      "2024     36.50\n",
      "Name: YoY_Change3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics by year and COVID period\n",
    "year_stats3 = df3.groupby('Year3')['Fare_Evasion'].agg(['mean', 'min', 'max']).reset_index()\n",
    "year_stats3.columns = ['Year3', 'Average_Evasion3', 'Min_Evasion3', 'Max_Evasion3']\n",
    "\n",
    "covid_stats3 = df3.groupby('COVID_Period3')['Fare_Evasion'].agg(['mean', 'min', 'max']).reset_index()\n",
    "covid_stats3.columns = ['COVID_Period3', 'Average_Evasion3', 'Min_Evasion3', 'Max_Evasion3']\n",
    "\n",
    "print(\"\\nYearly statistics generated\")\n",
    "print(\"\\nCOVID period statistics generated\")\n",
    "\n",
    "# Display the final data structure\n",
    "print(f\"\\nFinal cleaned data shape: {df3.shape}\")\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(df3.dtypes)\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "print(df3.head())\n",
    "\n",
    "# Compare statistics before and after outlier removal\n",
    "if 'df3_original' in locals():\n",
    "    print(\"\\nStatistics comparison before and after outlier removal:\")\n",
    "    print(\"Before removal:\", df3_original['Fare_Evasion'].describe().round(4))\n",
    "    print(\"After removal:\", df3['Fare_Evasion'].describe().round(4))\n",
    "\n",
    "print(\"\\nData Quality Report:\")\n",
    "print(f\"Total rows: {len(df3)}\")\n",
    "print(f\"Missing values: {df3.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nFare Evasion Summary Statistics:\")\n",
    "print(df3['Fare_Evasion'].describe())\n",
    "\n",
    "print(\"\\nYear-over-Year Summary:\")\n",
    "if 'YoY_Change3' in df3.columns:\n",
    "    print(df3.groupby('Year3')['YoY_Change3'].mean().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95140d1a-6be9-4319-9f47-c3626ef4a631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved cleaned data to 'cleaned_fare_evasion.csv'\n",
      "Saved summary statistics tables\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned data\n",
    "df3.to_csv(\"cleaned_fare_evasion.csv\", index=False)\n",
    "print(\"\\nSaved cleaned data to 'cleaned_fare_evasion.csv'\")\n",
    "\n",
    "# Save statistics tables\n",
    "year_stats3.to_csv(\"fare_evasion_yearly_stats.csv\", index=False)\n",
    "covid_stats3.to_csv(\"fare_evasion_covid_period_stats.csv\", index=False)\n",
    "print(\"Saved summary statistics tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f38615-17d5-40f8-bb11-5d0157067f9b",
   "metadata": {},
   "source": [
    "## **5. Station entry/exit cleansing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6485cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'Exit Allowed' values:\n",
      "Exit Allowed\n",
      "YES    2113\n",
      "NO        7\n",
      "Name: count, dtype: int64\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df4 = pd.read_csv(\"Station entry_exit.csv\")\n",
    "\n",
    "print(\"Original 'Exit Allowed' values:\")\n",
    "print(df4['Exit Allowed'].value_counts())\n",
    "print(\"Name: count, dtype: int64\")\n",
    "\n",
    "# Standardize column names\n",
    "df4.columns = df4.columns.str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Handle other missing values without touching Exit_Allowed\n",
    "text_columns4 = df4.select_dtypes(include=['object']).columns\n",
    "for col4 in text_columns4:\n",
    "    if col4 != 'Exit_Allowed': \n",
    "        df4[col4] = df4[col4].fillna('Unknown')\n",
    "\n",
    "# For numeric columns\n",
    "numeric_columns4 = df4.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col4 in numeric_columns4:\n",
    "    df4[col4] = df4[col4].fillna(df4[col4].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eefdeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted longitude and latitude from entrance_georeference\n"
     ]
    }
   ],
   "source": [
    "# Handle coordinate columns\n",
    "coordinate_cols4 = [col4 for col4 in df4.columns if ('entrance_georeference' in col4.lower() or \n",
    "                                                 'point' in col4.lower())]\n",
    "\n",
    "for col4 in coordinate_cols4:\n",
    "    if col4 in df4.columns and df4[col4].dtype == 'object':\n",
    "        if df4[col4].astype(str).str.contains('POINT').any():\n",
    "            extracted4 = df4[col4].astype(str).str.extract(r'POINT \\(([^)]+)\\)')\n",
    "            df4[[f'{col4}_lon', f'{col4}_lat']] = extracted4[0].str.split(' ', expand=True)\n",
    "            \n",
    "            df4[f'{col4}_lon'] = pd.to_numeric(df4[f'{col4}_lon'], errors='coerce')\n",
    "            df4[f'{col4}_lat'] = pd.to_numeric(df4[f'{col4}_lat'], errors='coerce')\n",
    "            print(f\"Extracted longitude and latitude from {col4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4943dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "before_rows4 = len(df4)\n",
    "df4 = df4.drop_duplicates()\n",
    "after_rows4 = len(df4)\n",
    "print(f\"Removed {before_rows4 - after_rows4} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64799afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 98 outliers in entrance_georeference_lon\n",
      "Identified 58 outliers in entrance_georeference_lat\n",
      "Removed 120 outlier rows\n"
     ]
    }
   ],
   "source": [
    "# Detect and remove outliers in coordinate columns\n",
    "outlier_cols4 = [col4 for col4 in df4.columns if ('_lon' in col4 or '_lat' in col4) and df4[col4].dtype != 'object']\n",
    "\n",
    "# Combined outlier flag\n",
    "df4['has_outlier4'] = False\n",
    "\n",
    "# Process each coordinate column\n",
    "for col4 in outlier_cols4:\n",
    "    if df4[col4].isna().all():\n",
    "        continue\n",
    "    \n",
    "    Q1_4 = df4[col4].quantile(0.25)\n",
    "    Q3_4 = df4[col4].quantile(0.75)\n",
    "    IQR4 = Q3_4 - Q1_4\n",
    "    \n",
    "    lower_bound4 = Q1_4 - 1.5 * IQR4\n",
    "    upper_bound4 = Q3_4 + 1.5 * IQR4\n",
    "    \n",
    "    outlier_flag4 = (df4[col4] < lower_bound4) | (df4[col4] > upper_bound4)\n",
    "    df4[f'{col4}_outlier4'] = outlier_flag4\n",
    "    df4['has_outlier4'] = df4['has_outlier4'] | outlier_flag4\n",
    "    \n",
    "    print(f\"Identified {outlier_flag4.sum()} outliers in {col4}\")\n",
    "\n",
    "# Remove outliers\n",
    "df4_no_outliers = df4[~df4['has_outlier4']]\n",
    "print(f\"Removed {len(df4) - len(df4_no_outliers)} outlier rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7362688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features\n",
    "if 'Entrance' in df4_no_outliers.columns and 'Exit_Allowed' in df4_no_outliers.columns:\n",
    "    df4_no_outliers['Access_Type4'] = 'Unknown'\n",
    "    df4_no_outliers.loc[(df4_no_outliers['Entrance'] == 'YES') & (df4_no_outliers['Exit_Allowed'] == 'YES'), 'Access_Type4'] = 'Both'\n",
    "    df4_no_outliers.loc[(df4_no_outliers['Entrance'] == 'YES') & (df4_no_outliers['Exit_Allowed'] != 'YES'), 'Access_Type4'] = 'Entrance Only'\n",
    "    df4_no_outliers.loc[(df4_no_outliers['Entrance'] != 'YES') & (df4_no_outliers['Exit_Allowed'] == 'YES'), 'Access_Type4'] = 'Exit Only'\n",
    "    df4_no_outliers.loc[(df4_no_outliers['Entrance'] != 'YES') & (df4_no_outliers['Exit_Allowed'] != 'YES'), 'Access_Type4'] = 'No Public Access'\n",
    "    \n",
    "    print(\"Created Access_Type4 feature\")\n",
    "    print(df4_no_outliers['Access_Type4'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af29ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final remove outlier flags\n",
    "outlier_flag_cols4 = [col4 for col4 in df4_no_outliers.columns if col4.endswith('_outlier4')]\n",
    "df4_no_outliers = df4_no_outliers.drop(columns=outlier_flag_cols4 + ['has_outlier4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f00666e-8af3-4f16-88d1-bcca63316c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final data shape: (2000, 17)\n",
      "\n",
      "Sample of cleaned data:\n",
      "  Division    Line Borough                 Stop_Name  Complex_ID  \\\n",
      "0      BMT  4th Av       B  Atlantic Av-Barclays Ctr         617   \n",
      "1      BMT  4th Av       B  Atlantic Av-Barclays Ctr         617   \n",
      "2      BMT  4th Av       B  Atlantic Av-Barclays Ctr         617   \n",
      "3      BMT  4th Av       B                  Union St          28   \n",
      "4      BMT  4th Av       B                  Union St          28   \n",
      "\n",
      "   Constituent_Station_Name  Station_ID GTFS_Stop_ID     Daytime_Routes  \\\n",
      "0  Atlantic Av-Barclays Ctr          27          R31  2 3 4 5 B D N Q R   \n",
      "1  Atlantic Av-Barclays Ctr          27          R31  2 3 4 5 B D N Q R   \n",
      "2  Atlantic Av-Barclays Ctr          27          R31  2 3 4 5 B D N Q R   \n",
      "3                  Union St          28          R32                  R   \n",
      "4                  Union St          28          R32                  R   \n",
      "\n",
      "  Entrance_Type Entry_Allowed Exit_Allowed  Entrance_Latitude  \\\n",
      "0         Stair           YES          YES          40.683905   \n",
      "1      Elevator           YES          YES          40.683805   \n",
      "2         Stair           YES          YES          40.683928   \n",
      "3         Stair           YES          YES          40.677154   \n",
      "4         Stair           YES          YES          40.677296   \n",
      "\n",
      "   Entrance_Longitude           entrance_georeference  \\\n",
      "0          -73.978879    POINT (-73.978879 40.683905)   \n",
      "1          -73.978487    POINT (-73.978487 40.683805)   \n",
      "2          -73.978412    POINT (-73.978412 40.683928)   \n",
      "3          -73.983430  POINT (-73.9834296 40.6771544)   \n",
      "4          -73.983336  POINT (-73.9833364 40.6772958)   \n",
      "\n",
      "   entrance_georeference_lon  entrance_georeference_lat  \n",
      "0                 -73.978879                  40.683905  \n",
      "1                 -73.978487                  40.683805  \n",
      "2                 -73.978412                  40.683928  \n",
      "3                 -73.983430                  40.677154  \n",
      "4                 -73.983336                  40.677296  \n",
      "\n",
      "Data Quality Report:\n",
      "Total rows: 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0\n",
      "Duplicate row count: 0\n",
      "\n",
      "Saved cleaned data to 'cleaned_station_entry_exit.csv'\n"
     ]
    }
   ],
   "source": [
    "# Final data quality report\n",
    "print(\"\\nFinal data shape:\", df4_no_outliers.shape)\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "print(df4_no_outliers.head())\n",
    "\n",
    "print(\"\\nData Quality Report:\")\n",
    "print(f\"Total rows: {len(df4_no_outliers)}\")\n",
    "print(f\"Missing values: {df4_no_outliers.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate row count: {df4_no_outliers.duplicated().sum()}\")\n",
    "\n",
    "df4_no_outliers.to_csv(\"cleaned_station_entry_exit.csv\", index=False)\n",
    "print(\"\\nSaved cleaned data to 'cleaned_station_entry_exit.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75dd3c3-a533-45d6-b58a-720cdda81bd8",
   "metadata": {},
   "source": [
    "## **6. Traffic Violations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ac31683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original column names: ['Violation Charged Code', 'Violation Description', 'Violation Year', 'Violation Month', 'Violation Day of Week', 'Age at Violation', 'Gender', 'State of License', 'Police Agency', 'Court', 'Source']\n",
      "\n",
      "Original data types:\n",
      "Violation Charged Code     object\n",
      "Violation Description      object\n",
      "Violation Year              int64\n",
      "Violation Month             int64\n",
      "Violation Day of Week      object\n",
      "Age at Violation          float64\n",
      "Gender                     object\n",
      "State of License           object\n",
      "Police Agency              object\n",
      "Court                      object\n",
      "Source                     object\n",
      "dtype: object\n",
      "\n",
      "Standardized column names: ['Violation_Charged_Code', 'Violation_Description', 'Violation_Year', 'Violation_Month', 'Violation_Day_of_Week', 'Age_at_Violation', 'Gender', 'State_of_License', 'Police_Agency', 'Court', 'Source']\n"
     ]
    }
   ],
   "source": [
    "df5 = pd.read_csv(\"Traffic violations.csv\")\n",
    "\n",
    "print(\"Original column names:\", df5.columns.tolist())\n",
    "print(\"\\nOriginal data types:\")\n",
    "print(df5.dtypes)\n",
    "\n",
    "# Standardize column names\n",
    "df5.columns = df5.columns.str.strip().str.replace(' ', '_')\n",
    "print(\"\\nStandardized column names:\", df5.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de86af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 163950 missing values before cleaning\n",
      "Missing values by column (top 5):\n",
      "Age_at_Violation          163950\n",
      "Violation_Charged_Code         0\n",
      "Violation_Description          0\n",
      "Violation_Year                 0\n",
      "Violation_Month                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_before5 = df5.isnull().sum()\n",
    "total_missing_before5 = missing_before5.sum()\n",
    "print(f\"\\nFound {total_missing_before5} missing values before cleaning\")\n",
    "print(\"Missing values by column (top 5):\")\n",
    "print(missing_before5.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2938dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and handle inconsistent values\n",
    "if 'State_of_X' in df5.columns:\n",
    "    print(\"\\nUnique values in State_of_X:\")\n",
    "    print(df5['State_of_X'].value_counts())\n",
    "    \n",
    "    # Standardize state names\n",
    "    state_mapping5 = {\n",
    "        'NEW YORK': 'NY',\n",
    "        'NEW JERSEY': 'NJ',\n",
    "        'PENNSYLVANIA': 'PA',\n",
    "        'UNKNOWN': 'UNKNOWN'\n",
    "    }\n",
    "    df5['State_Standardized5'] = df5['State_of_X'].map(state_mapping5)\n",
    "    print(\"\\nStandardized states:\")\n",
    "    print(df5['State_Standardized5'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd8cf738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values in Violation_Charged_Code with '1180D'\n",
      "Filled missing values in Violation_Description with 'SPEED IN ZONE'\n",
      "Filled missing values in Violation_Day_of_Week with 'WEDNESDAY'\n",
      "Filled missing values in Gender with 'M'\n",
      "Filled missing values in Police_Agency with 'NYC POLICE DEPT'\n",
      "Filled missing values in Court with 'NASSAU TRAFFIC/PARKING AGENCY'\n",
      "Filled missing values in Source with 'TSLED'\n",
      "Filled missing values in Violation_Year with median\n",
      "Filled missing values in Violation_Month with median\n",
      "Filled missing values in Age_at_Violation with median\n",
      "\n",
      "Filled 163950 missing values\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "# For categorical columns\n",
    "cat_columns5 = df5.select_dtypes(include=['object']).columns\n",
    "for col5 in cat_columns5:\n",
    "    if 'state' in col5.lower():\n",
    "        df5[col5] = df5[col5].fillna('UNKNOWN')\n",
    "    else:\n",
    "        most_frequent5 = df5[col5].mode()[0]\n",
    "        df5[col5] = df5[col5].fillna(most_frequent5)\n",
    "        print(f\"Filled missing values in {col5} with '{most_frequent5}'\")\n",
    "\n",
    "# For numeric columns\n",
    "num_columns5 = df5.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col5 in num_columns5:\n",
    "    df5[col5] = df5[col5].fillna(df5[col5].median())\n",
    "    print(f\"Filled missing values in {col5} with median\")\n",
    "\n",
    "missing_after5 = df5.isnull().sum().sum()\n",
    "print(f\"\\nFilled {total_missing_before5 - missing_after5} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10ea82dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Violation_Year to numeric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\guanq\\AppData\\Local\\Temp\\ipykernel_35952\\3121041409.py:10: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df5['Age_Numeric5'] = df5['Age_at_Violation'].str.extract('(\\d+)').astype(float)\n",
      "C:\\Users\\guanq\\AppData\\Local\\Temp\\ipykernel_35952\\3121041409.py:11: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df5['Gender_Extracted5'] = df5['Age_at_Violation'].str.extract('(\\d+\\s*(\\w))').iloc[:, 1]\n"
     ]
    }
   ],
   "source": [
    "# Convert data types\n",
    "date_columns5 = [col5 for col5 in df5.columns if 'date' in col5.lower() or 'year' in col5.lower()]\n",
    "for col5 in date_columns5:\n",
    "    if 'year' in col5.lower() and df5[col5].dtype != 'datetime64[ns]':\n",
    "        df5[col5] = pd.to_numeric(df5[col5], errors='coerce')\n",
    "        print(f\"Converted {col5} to numeric\")\n",
    "\n",
    "if 'Age_at_Violation' in df5.columns:\n",
    "    if df5['Age_at_Violation'].dtype == 'object':\n",
    "        df5['Age_Numeric5'] = df5['Age_at_Violation'].str.extract('(\\d+)').astype(float)\n",
    "        df5['Gender_Extracted5'] = df5['Age_at_Violation'].str.extract('(\\d+\\s*(\\w))').iloc[:, 1]\n",
    "        print(\"Extracted numeric age and gender\")\n",
    "    else:\n",
    "        df5['Age_Numeric5'] = df5['Age_at_Violation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf73fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed 1157935 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Identify and remove duplicates\n",
    "before_rows5 = len(df5)\n",
    "df5 = df5.drop_duplicates()\n",
    "after_rows5 = len(df5)\n",
    "print(f\"\\nRemoved {before_rows5 - after_rows5} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef237afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identified 59042 outliers in Age_Numeric5\n",
      "Age outlier bounds: -5.0 - 75.0\n",
      "Age outliers:\n",
      "Age_Numeric5\n",
      "76.0    10922\n",
      "77.0     8763\n",
      "78.0     7635\n",
      "79.0     6614\n",
      "80.0     5280\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Removed 59042 outlier rows\n",
      "Data shape after removing outliers: (8844876, 13)\n",
      "\n",
      "Age distribution before removing outliers:\n",
      "count    8.903918e+06\n",
      "mean     3.649193e+01\n",
      "std      1.393770e+01\n",
      "min      1.600000e+01\n",
      "25%      2.500000e+01\n",
      "50%      3.300000e+01\n",
      "75%      4.500000e+01\n",
      "max      9.500000e+01\n",
      "Name: Age_Numeric5, dtype: float64\n",
      "\n",
      "Age distribution after removing outliers:\n",
      "count    8.844876e+06\n",
      "mean     3.620253e+01\n",
      "std      1.352154e+01\n",
      "min      1.600000e+01\n",
      "25%      2.500000e+01\n",
      "50%      3.300000e+01\n",
      "75%      4.500000e+01\n",
      "max      7.500000e+01\n",
      "Name: Age_Numeric5, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Detect outliers in numeric columns\n",
    "if 'Age_Numeric5' in df5.columns:\n",
    "    if not df5['Age_Numeric5'].isna().all():\n",
    "        Q1_5 = df5['Age_Numeric5'].quantile(0.25)\n",
    "        Q3_5 = df5['Age_Numeric5'].quantile(0.75)\n",
    "        IQR5 = Q3_5 - Q1_5\n",
    "        \n",
    "        lower_bound5 = Q1_5 - 1.5 * IQR5\n",
    "        upper_bound5 = Q3_5 + 1.5 * IQR5\n",
    "        \n",
    "        # Create outlier flag\n",
    "        outlier_flag5 = (df5['Age_Numeric5'] < lower_bound5) | (df5['Age_Numeric5'] > upper_bound5)\n",
    "        df5['Age_outlier5'] = outlier_flag5\n",
    "        \n",
    "        print(f\"\\nIdentified {outlier_flag5.sum()} outliers in Age_Numeric5\")\n",
    "        print(f\"Age outlier bounds: {lower_bound5:.1f} - {upper_bound5:.1f}\")\n",
    "        print(\"Age outliers:\")\n",
    "        print(df5[outlier_flag5]['Age_Numeric5'].value_counts().head())\n",
    "\n",
    "# Remove outliers\n",
    "if 'Age_outlier5' in df5.columns:\n",
    "    df5_original = df5.copy() \n",
    "    df5_no_outliers = df5[~df5['Age_outlier5']]\n",
    "    print(f\"\\nRemoved {len(df5) - len(df5_no_outliers)} outlier rows\")\n",
    "    print(f\"Data shape after removing outliers: {df5_no_outliers.shape}\")\n",
    "    \n",
    "    # Compare age distribution before and after\n",
    "    print(\"\\nAge distribution before removing outliers:\")\n",
    "    print(df5['Age_Numeric5'].describe())\n",
    "    print(\"\\nAge distribution after removing outliers:\")\n",
    "    print(df5_no_outliers['Age_Numeric5'].describe())\n",
    "\n",
    "    df5 = df5_no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e116030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Age_Group5 feature\n"
     ]
    }
   ],
   "source": [
    "# Add derived features for analysis\n",
    "if 'Violation_Date' in df5.columns and pd.api.types.is_datetime64_dtype(df5['Violation_Date']):\n",
    "    df5['Day_of_Week5'] = df5['Violation_Date'].dt.day_name()\n",
    "    print(\"\\nCreated Day_of_Week5 feature\")\n",
    "elif 'Violation_3' in df5.columns:  \n",
    "    day_mapping5 = {\n",
    "        1: 'Monday',\n",
    "        2: 'Tuesday',\n",
    "        3: 'Wednesday',\n",
    "        4: 'Thursday',\n",
    "        5: 'Friday',\n",
    "        6: 'Saturday',\n",
    "        7: 'Sunday'\n",
    "    }\n",
    "    df5['Day_of_Week5'] = df5['Violation_3'].map(day_mapping5)\n",
    "    print(\"\\nCreated Day_of_Week5 feature from Violation_3\")\n",
    "\n",
    "# Age groups for analysis\n",
    "if 'Age_Numeric5' in df5.columns:\n",
    "    bins5 = [0, 18, 25, 35, 45, 55, 65, 100]\n",
    "    labels5 = ['Under 18', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "    df5['Age_Group5'] = pd.cut(df5['Age_Numeric5'], bins=bins5, labels=labels5)\n",
    "    print(\"Created Age_Group5 feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c4bf76f-f029-4b8e-a2a9-10e15975bae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed 1 temporary columns\n"
     ]
    }
   ],
   "source": [
    "# Final cleanup\n",
    "columns_to_drop5 = ['Age_outlier5'] if 'Age_outlier5' in df5.columns else []\n",
    "df5 = df5.drop(columns=columns_to_drop5)\n",
    "print(f\"\\nRemoved {len(columns_to_drop5)} temporary columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6e0e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final data shape: (8844876, 13)\n",
      "\n",
      "Column Data Types:\n",
      "Violation_Charged_Code      object\n",
      "Violation_Description       object\n",
      "Violation_Year               int64\n",
      "Violation_Month              int64\n",
      "Violation_Day_of_Week       object\n",
      "Age_at_Violation           float64\n",
      "Gender                      object\n",
      "State_of_License            object\n",
      "Police_Agency               object\n",
      "Court                       object\n",
      "Source                      object\n",
      "Age_Numeric5               float64\n",
      "Age_Group5                category\n",
      "dtype: object\n",
      "\n",
      "Sample of cleaned data:\n",
      "  Violation_Charged_Code      Violation_Description  Violation_Year  \\\n",
      "0                  403A1       FLD YIELD PEDEST NYC            2020   \n",
      "1                  1110A   DISOBEYED TRAFFIC DEVICE            2020   \n",
      "2                1225C2A  OPERATING MV MOBILE PHONE            2020   \n",
      "3                  4014B       COM VEH ON PKWAY NYC            2020   \n",
      "4                1111D1N               NYC REDLIGHT            2020   \n",
      "\n",
      "   Violation_Month Violation_Day_of_Week  Age_at_Violation Gender  \\\n",
      "0                3                FRIDAY              56.0      M   \n",
      "1                3                FRIDAY              29.0      F   \n",
      "2                3                FRIDAY              28.0      M   \n",
      "3                3                FRIDAY              30.0      M   \n",
      "4                3                FRIDAY              56.0      M   \n",
      "\n",
      "  State_of_License    Police_Agency               Court Source  Age_Numeric5  \\\n",
      "0         NEW YORK  NYC POLICE DEPT           BRONX TVB    TVB          56.0   \n",
      "1       NEW JERSEY  NYC POLICE DEPT  BROOKLYN SOUTH TVB    TVB          29.0   \n",
      "2         NEW YORK  NYC POLICE DEPT  BROOKLYN NORTH TVB    TVB          28.0   \n",
      "3         NEW YORK  NYC POLICE DEPT           BRONX TVB    TVB          30.0   \n",
      "4         NEW YORK  NYC POLICE DEPT           BRONX TVB    TVB          56.0   \n",
      "\n",
      "  Age_Group5  \n",
      "0      55-64  \n",
      "1      25-34  \n",
      "2      25-34  \n",
      "3      25-34  \n",
      "4      55-64  \n",
      "\n",
      "Data Quality Report:\n",
      "Total rows: 8844876\n",
      "Missing values: 0\n",
      "Duplicate row count: 0\n",
      "\n",
      "Saved cleaned data to 'cleaned_traffic_violations.csv'\n"
     ]
    }
   ],
   "source": [
    "# Final data quality report\n",
    "print(\"\\nFinal data shape:\", df5.shape)\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(df5.dtypes)\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "print(df5.head())\n",
    "\n",
    "print(\"\\nData Quality Report:\")\n",
    "print(f\"Total rows: {len(df5)}\")\n",
    "print(f\"Missing values: {df5.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate row count: {df5.duplicated().sum()}\")\n",
    "\n",
    "df5.to_csv(\"cleaned_traffic_violations.csv\", index=False)\n",
    "print(\"\\nSaved cleaned data to 'cleaned_traffic_violations.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d813999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
